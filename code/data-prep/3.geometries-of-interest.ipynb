{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d404bf",
   "metadata": {},
   "source": [
    "In this script, we select only the cities that contain one or more dams to reduce the complexity of the analysis that will follow and export the list as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f743e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geobr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from shapely.geometry import Point, Polygon\n",
    "import warnings; warnings.filterwarnings('ignore', message='.*initial implementation of Parquet.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd70535d",
   "metadata": {},
   "source": [
    "#### General helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b158266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dams():\n",
    "    '''\n",
    "    Reads the dam safety dataset using\n",
    "    the appropriate configurations.\n",
    "    '''\n",
    "\n",
    "    dams = pd.read_csv(\"../../data/brazil/snisb/dam-report-07022021.csv\", encoding='Latin5', sep=';', skiprows=[0,1])\n",
    "    return dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "796ba519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gdf(df):\n",
    "    '''\n",
    "    Converts the dataframe to a geodaframe\n",
    "    using the columns Longitude and Latitude\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    df -> The dam safety dataframe\n",
    "    '''\n",
    "    \n",
    "    df['geometry'] = df.apply(lambda row: Point(row.longitude, row.latitude), axis=1)\n",
    "    \n",
    "    df = gpd.GeoDataFrame(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5580edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crs_to_area(gdf):\n",
    "    '''\n",
    "    Converts the CRS for equal\n",
    "    area calculations.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    gdf -> A geodataframe\n",
    "    '''\n",
    "    \n",
    "    return gdf.to_crs('''PROJCS[\"Brasil_Albers_Equal_Area\",GEOGCS[\"GCS_WGS_1984\",DATUM[\"D_SIRGAS_2000\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Albers\"],PARAMETER[\"false_easting\",5000000.0],PARAMETER[\"false_northing\",10000000.0],PARAMETER[\"central_meridian\",-54.0],PARAMETER[\"standard_parallel_1\",-2.0],PARAMETER[\"standard_parallel_2\",-22.0],PARAMETER[\"latitude_of_origin\",-12.0],UNIT[\"Meter\",1.0]]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c665aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crs_to_coords(gdf):\n",
    "    '''\n",
    "    Converts the CRS of the geodataframe\n",
    "    to the Brazilian standard for geogra-\n",
    "    phic projections.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    gdf -> A geodataframe\n",
    "    '''\n",
    "    \n",
    "    return gdf.to_crs(\"EPSG:4674\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1b634f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_buffer(gdf, r):\n",
    "    '''\n",
    "    Creates a buffer around the\n",
    "    geometries of the given gdf\n",
    "    with a r radius.\n",
    "    '''\n",
    "    \n",
    "    gdf.geometry = gdf.geometry.buffer(r)\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a98688",
   "metadata": {},
   "source": [
    "#### Data cleaning functions\n",
    "The Brazilian National Dam Safety Information System data is a mess. Let's clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d353b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_columns(df):\n",
    "    '''\n",
    "    Remove special characters from the column\n",
    "    names and makes them all lowercase. \n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    df -> The dam safety dataframe\n",
    "    '''\n",
    "    \n",
    "    df.columns = df.columns.map(unidecode)\n",
    "    df.columns = df.columns.map(lambda x: x.lower())\n",
    "    df.columns = df.columns.map(lambda x: x.strip())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9991a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_separators(df):\n",
    "    '''\n",
    "    The Latitude and Longitude columns in the dataframe\n",
    "    are currently stored as strings with a ',' as the decimal\n",
    "    separator. This function changes the separator to '.' and\n",
    "    casts it to float.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    df -> The dam safety dataframe\n",
    "    '''\n",
    "    \n",
    "    df.latitude = df.latitude.str.replace(\",\", \".\").astype(float)\n",
    "    df.longitude = df.longitude.str.replace(\",\", \".\").astype(float)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e875c3",
   "metadata": {},
   "source": [
    "#### Dam classification functions\n",
    "Let's mark the dams that we consider specially dangerous â€“ that is, those that have both a high structural risk and a high potential damage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b1d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_risky_dams(df):\n",
    "    '''\n",
    "    Marks the dams that have both a\n",
    "    high risk category and a high\n",
    "    potential damage so we can proceed \n",
    "    in the analysis.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    df -> The dam safety dataframe\n",
    "    '''\n",
    "    \n",
    "    condition = (df.categoria_de_risco == 'Alto') & (df.dano_potencial_associado == 'Alto')\n",
    "    \n",
    "    df['high_risk_high_damage'] = np.where(condition, True, False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970dbb38",
   "metadata": {},
   "source": [
    "#### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3808627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dams():\n",
    "    '''\n",
    "    Runs all the functions to prepare\n",
    "    the dam safety dataset sequentially.\n",
    "    '''\n",
    "    \n",
    "    dams = read_dams()\n",
    "    dams = standardize_columns(dams)\n",
    "    dams = fix_separators(dams)\n",
    "    dams = make_gdf(dams)\n",
    "    dams = classify_risky_dams(dams)\n",
    "    dams = dams[dams.high_risk_high_damage] # selects using boolean mask\n",
    "    dams = dams.set_crs(\"EPSG:4674\") # Brazilian standard projection\n",
    "    dams = crs_to_area(dams)\n",
    "    dams = create_buffer(dams, 1000)\n",
    "    \n",
    "    return dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "590031f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cities_of_interest(dams):\n",
    "    '''\n",
    "    Runs all the functions to download and\n",
    "    select the cities of interest.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    dams -> the dataframe containing information\n",
    "    about dams in Brazil\n",
    "    '''\n",
    "    \n",
    "    # Downloads and saves city data\n",
    "    cities = geobr.read_municipality(code_muni=\"all\", year=2020)\n",
    "    cities = cities.to_crs(dams.crs)\n",
    "    cities.to_feather(\"../../data/brazil/cities/cities.feather\")\n",
    "    \n",
    "    # Selects the cities that intersect with the radiuses\n",
    "    cities_of_interest = gpd.sjoin(cities, dams, how='inner', op='intersects')\n",
    "    \n",
    "    # Keeps only the id columns and only the unique entries\n",
    "    cities_of_interest = cities_of_interest.drop_duplicates(subset='code_muni')\n",
    "    \n",
    "    # Removes the information associated with the dams\n",
    "    cities_of_interest = cities_of_interest.drop(columns=['index_right', 'codigo_snisb',\n",
    "       'nome_da_barragem', 'nome_secundario', 'uso_principal', 'uf',\n",
    "       'municipio', 'categoria_de_risco', 'dano_potencial_associado',\n",
    "       'nome_do_empreendedor', 'orgao_fiscalizador',\n",
    "       'codigo_barragem_fiscalizador', 'regulada_pela_pnsb',\n",
    "       'numero_da_autorizacao', 'possui_pae', 'possui_plano_de_seguranassa',\n",
    "       'possui_revisao_periodica', 'data_da_ultima_fiscalizacao',\n",
    "       'barragem_autuada', 'altura_fundacao_m', 'altura_terreno_m',\n",
    "       'capacidade_hm3', 'comprimento_coroamento_m', 'tipo_de_material',\n",
    "       'uso_complementar', 'classe_de_residuo', 'curso_dagua_barrado',\n",
    "       'regiao_hidrografica', 'unidade_de_gestao', 'dominio',\n",
    "       'data_da_ultima_inspecao', 'tipo_da_ultima_inspecao',\n",
    "       'nivel_de_perigo_global', 'possui_eclusa', 'fase_da_vida', 'latitude',\n",
    "       'longitude', 'completude', 'high_risk_high_damage'])\n",
    "    \n",
    "    # Saves file\n",
    "    cities_of_interest.to_feather(\"../../data/brazil/cities/cities-of-interest.feather\")\n",
    "    \n",
    "    return cities_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c2cc18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_of_interest(cities_of_interest):\n",
    "    '''\n",
    "    Uses r-trees and spatial indexes to efficiently\n",
    "    detect which squares in the statistical grid \n",
    "    intersect with our area of interest.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    cities_of_interest -> The dataframe containing the cities of interest\n",
    "    '''\n",
    "    \n",
    "    # Reads data\n",
    "    grid = gpd.read_feather(\"../../data/brazil/grade/feather/concatenated/populated.feather\")\n",
    "    \n",
    "    # Converts CRS\n",
    "    grid = crs_to_area(grid)\n",
    "    \n",
    "    # Creates a polygon which is the union of the cities of interest\n",
    "    union = cities_of_interest.geometry.unary_union\n",
    "    \n",
    "    # Cut the union in many smaller bounding boxes\n",
    "    bboxes = ox.utils_geo._quadrat_cut_geometry(union, quadrat_width=100000) # 100km side for each box\n",
    "    \n",
    "    # Create a spatial index for the grid\n",
    "    sindex = grid.sindex\n",
    "    \n",
    "    # Find the squares that intersect with each bounding box and save their ids\n",
    "    grid_of_interest = pd.DataFrame()\n",
    "    for index, bbox in enumerate(bboxes):\n",
    "\n",
    "        print(f\"{index + 1} of {len(bboxes)}\", end='\\r')\n",
    "\n",
    "        # Find approximate matches with r-tree, then precise matches from those approximate ones\n",
    "        possible_matches_index = list(sindex.intersection(bbox.bounds))\n",
    "        possible_matches = grid.iloc[possible_matches_index]\n",
    "\n",
    "        precise_matches = possible_matches[possible_matches.intersects(bbox)]\n",
    "        grid_of_interest = grid_of_interest.append(precise_matches)\n",
    "        \n",
    "    # Keeps columns of interest\n",
    "    grid_of_interest = grid_of_interest[['ID_UNICO', 'QUADRANTE', 'MASC', 'FEM', 'POP',\n",
    "       'DOM_OCU', 'geometry']]\n",
    "    \n",
    "    # No duplicates\n",
    "    grid_of_interest = grid_of_interest.drop_duplicates(subset='ID_UNICO')\n",
    "    \n",
    "    # Saves\n",
    "    grid_of_interest.to_feather(\"../../data/brazil/grade/feather/concatenated/grid-of-interest.feather\")\n",
    "    \n",
    "    return grid_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7337928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracts_of_interest(grid_of_interest):\n",
    "    '''\n",
    "    Runs all the functions to read and\n",
    "    select the tracts of interest.\n",
    "    ---\n",
    "    Parameters:\n",
    "    \n",
    "    grid_of_interest -> The dataframe\n",
    "    containing the squares of interest\n",
    "    '''\n",
    "    # Reads tract data and keeps only those that intersect with the cities\n",
    "    tracts = gpd.read_feather(\"../../data/brazil/censo/combined/combined.feather\")\n",
    "    tracts = tracts.to_crs(grid_of_interest.crs)\n",
    "    \n",
    "    # A zero side buffer is needed to correct self-intercepting geometries\n",
    "    tracts.geometry = tracts.geometry.buffer(0) \n",
    "    \n",
    "    # Spatial join to keep only tracts in the cities of interest\n",
    "    tracts_of_interest = gpd.sjoin(tracts, grid_of_interest, how='inner', op='intersects')\n",
    "    \n",
    "    # There is some overlap on neighboring tracts (probably from simplified geometries) that we must drop\n",
    "    tracts_of_interest = tracts_of_interest.drop_duplicates(subset='code_tract')\n",
    "    \n",
    "    # Keeps only columns that matter\n",
    "    tracts_of_interest = tracts_of_interest[['code_tract', 'geometry', 'total_permanent_households',\n",
    "       'permanent_household_nominal_mean_income', 'total_private_households',\n",
    "       'private_households_under_minimum_wage', 'total_residents',\n",
    "       'white_residents', 'black_residents', 'yellow_residents',\n",
    "       'pardo_residents', 'indigenous_residents', 'literate_residents',\n",
    "       'pct_private_households_under_minimum_wage', 'pct_literate_residents',\n",
    "       'pct_black_and_pardo_residents']]\n",
    "    \n",
    "    # Saves\n",
    "    tracts_of_interest.to_feather(\"../../data/brazil/censo/combined/tracts-of-interest.feather\")\n",
    "\n",
    "    return tracts_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ee4543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Processing dams\")\n",
    "    dams = get_dams()\n",
    "    \n",
    "    print(\"Processing cities of interest\")\n",
    "    cities_of_interest = get_cities_of_interest(dams)\n",
    "    \n",
    "    print(\"Processing grid of interest\")\n",
    "    grid_of_interest = get_grid_of_interest(cities_of_interest)\n",
    "    \n",
    "    print(\"Processing tracts of interest\")\n",
    "    tracts_of_interest = get_tracts_of_interest(grid_of_interest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feffda35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dams\n",
      "Processing cities of interest\n",
      "Processing grid of interest\n",
      "Processing tracts of interest\n",
      "CPU times: user 2min 35s, sys: 10.9 s, total: 2min 46s\n",
      "Wall time: 2min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
